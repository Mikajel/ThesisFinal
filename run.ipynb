{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, path, listdir\n",
    "import warnings\n",
    "import pickle\n",
    "%load_ext autoreload\n",
    "from importlib import reload\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "chdir(\n",
    "    path.join(\n",
    "        path.expanduser('~'), \n",
    "        'PycharmProjects', \n",
    "        'DiplomaThesis'\n",
    "    )\n",
    ")\n",
    "print(getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from '/Users/michalfasanek/PycharmProjects/DiplomaThesis/config.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import config as cfg\n",
    "from os import path, getcwd, listdir\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from random import shuffle\n",
    "from collections import OrderedDict\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "reload(cfg)"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "SUPPORT FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shuffled_train_valid_test(\n",
    "    filepaths: [str], \n",
    "    ratios: tuple, \n",
    "    required_sample_amount: int,\n",
    "    flag_oversample: bool,\n",
    "    flag_undersample: bool,\n",
    "    sampling_target_amount: int):\n",
    "    \n",
    "    all_user_vectors = []\n",
    "    \n",
    "    partners_list = []   \n",
    "    partner_numerosities = {}\n",
    "    \n",
    "    for filepath in filepaths:\n",
    "        with open(path.join(getcwd(), cfg.dir_sorted_partner_vectors_input_with_partners, filepath), 'rb') as vector_file:\n",
    "            \n",
    "            current_file_vectors = pickle.load(vector_file)\n",
    "            \n",
    "            if len(current_file_vectors) >= required_sample_amount:\n",
    "                \n",
    "                if flag_undersample:\n",
    "                    # undersample\n",
    "                    if len(current_file_vectors) > sampling_target_amount:\n",
    "                        current_file_vectors = current_file_vectors[:sampling_target_amount]\n",
    "                        \n",
    "                all_user_vectors += current_file_vectors\n",
    "            \n",
    "            partner_id = int(filepath.split('_')[-1])\n",
    "            partners_list.append(partner_id)\n",
    "            partner_vector_amount = len(current_file_vectors)\n",
    "            partner_numerosities[partner_id] = partner_vector_amount\n",
    "            \n",
    "    # SMOTE\n",
    "    if flag_oversample:\n",
    "        \n",
    "        all_vector_inputs = [vector_pair[0] for vector_pair in all_user_vectors]\n",
    "        all_vector_targets = [vector_pair[1] for vector_pair in all_user_vectors]\n",
    "        \n",
    "        inputs_res, targets_res = SMOTE(kind='svm').fit_sample(all_vector_inputs, all_vector_targets)\n",
    "        \n",
    "        all_user_vectors = [(inputs_res[index], targets_res[index]) for index in range(len(inputs_res))]\n",
    "            \n",
    "    # SMOTE\n",
    "            \n",
    "    max_vector_amount = max(partner_numerosities.values())    \n",
    "    partners_list.sort()\n",
    "    \n",
    "    class_weights = []\n",
    "        \n",
    "    for partner_id in partners_list:\n",
    "        \n",
    "        current_class_weight = (1 - partner_numerosities[partner_id]/(max_vector_amount*2))**4\n",
    "        class_weights.append(\n",
    "            current_class_weight\n",
    "        )\n",
    "        if partner_numerosities[partner_id] > 100:\n",
    "            print('Partner vectors amount: {}'.format(partner_numerosities[partner_id]))\n",
    "            print('Max vector amount*2: {}'.format(max_vector_amount*2))\n",
    "            print('Result class weight: {}\\n'.format(current_class_weight))\n",
    "        \n",
    "    \n",
    "    print(class_weights)\n",
    "            \n",
    "    shuffle(all_user_vectors)\n",
    "    \n",
    "    train_ratio, valid_ratio, test_ratio = ratios\n",
    "    n_vectors = len(all_user_vectors)\n",
    "    \n",
    "    train = all_user_vectors[:int(n_vectors*train_ratio)]\n",
    "    valid = all_user_vectors[int(n_vectors*train_ratio):int(n_vectors*(train_ratio+valid_ratio))]\n",
    "    test  = all_user_vectors[int(n_vectors*(train_ratio+valid_ratio)):]\n",
    "    \n",
    "    print('Train vectors: {}'.format(len(train)))\n",
    "    print('Valid vectors: {}'.format(len(valid)))\n",
    "    print('Test  vectors: {}'.format(len(test )))\n",
    "    \n",
    "    x_train = np.array([np.array(vector[0]) for vector in train])\n",
    "    y_train = np.array([np.array(vector[1]) for vector in train])\n",
    "    \n",
    "    x_valid = np.array([np.array(vector[0]) for vector in valid])\n",
    "    y_valid = np.array([np.array(vector[1]) for vector in valid])\n",
    "    \n",
    "    x_test  = np.array([np.array(vector[0]) for vector in  test])\n",
    "    y_test  = np.array([np.array(vector[1]) for vector in  test])\n",
    "    \n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test, class_weights\n",
    "\n",
    "def show_dataset_class_distribution(filepaths: [str]):\n",
    "    \n",
    "    reverse_partners_count = {}\n",
    "    \n",
    "    input_batch_filepaths = sorted(\n",
    "        [path.join(getcwd(), cfg.dir_sorted_partner_vectors_input_with_partners, filename)\n",
    "         for filename in listdir(path.join(getcwd(), cfg.dir_sorted_partner_vectors_input_with_partners))]\n",
    "    )\n",
    "    \n",
    "    for filepath in filepaths:\n",
    "        with open(path.join(getcwd(), cfg.dir_partners_grouped_vectors, filepath), 'rb') as vector_file:\n",
    "            \n",
    "            partner_id = filepath.split('_')[-1]\n",
    "            partner_vector_count = len(pickle.load(vector_file))\n",
    "            \n",
    "            try:\n",
    "                reverse_partners_count[partner_vector_count].append(partner_id)\n",
    "            except KeyError:\n",
    "                reverse_partners_count[partner_vector_count] = [partner_id]\n",
    "                \n",
    "    # ordering dict by vector amount\n",
    "    ordered = OrderedDict()\n",
    "    \n",
    "    while len(reverse_partners_count.keys()):\n",
    "        \n",
    "        max_vectors = max(reverse_partners_count.keys())\n",
    "        max_partners = reverse_partners_count[max_vectors]\n",
    "        \n",
    "        ordered[max_vectors] = max_partners\n",
    "        \n",
    "        reverse_partners_count.pop(max_vectors, None)\n",
    "    \n",
    "    for vector_count, partner_ids in ordered.items():\n",
    "        print('Samples: {}'.format(vector_count).ljust(15, ' ') + 'Partners amount: {}'.format(len(partner_ids)))\n",
    "       \n",
    "\n",
    "def get_input_target_lengths(check_print: bool=False) -> (int, int, int):\n",
    "    \"\"\"\n",
    "    Get vector shapes\n",
    "    :param check_print:\n",
    "    bool, print shapes into console\n",
    "    :return:\n",
    "    size of input vector, number of steps, size of output vector\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path.join(\n",
    "        getcwd(), \n",
    "        cfg.dir_sorted_partner_vectors_input_with_partners, \n",
    "        cfg.sorted_heavy_partner_vectors_baseline_filename + '13295'\n",
    "    ), 'rb') as vector_file:\n",
    "\n",
    "        all_users_vectors = pickle.load(vector_file)\n",
    "\n",
    "    sample_user_tuple = all_users_vectors[0]\n",
    "\n",
    "    if check_print:\n",
    "        print('Amount of sample users:\\n{}'.format(len(all_users_vectors)))\n",
    "        print('Amount of input time events:\\n{}\\n\\n'.format(len(all_users_vectors[0][0])))\n",
    "\n",
    "        for pair in all_users_vectors:\n",
    "            print('Length of single input vector: {}'.format(len(pair[0][0])))\n",
    "            print('Length of single target vector:{}'.format(len(pair[1])))\n",
    "\n",
    "        print('Sample user tuple: ')\n",
    "        print('Inputs:')\n",
    "        print(sample_user_tuple[0])\n",
    "        print('Target:')\n",
    "        print(sample_user_tuple[1])\n",
    "        print('Ones in sample target:')\n",
    "        print('{}'.format(sum(sample_user_tuple[1])))\n",
    "\n",
    "    return len(sample_user_tuple[0][0]), len(all_users_vectors[0][0]), len(sample_user_tuple[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split_ratio = (0.7, 0.2, 0.1)\n",
    "flag_oversample = False\n",
    "flag_undersample = True\n",
    "sampling_target_amount = 100\n",
    "required_sample_amount = 50\n",
    "\n",
    "input_batch_filepaths = sorted(\n",
    "    [path.join(getcwd(), cfg.dir_sorted_partner_vectors_input_with_partners, filename)\n",
    "     for filename in listdir(path.join(getcwd(), cfg.dir_sorted_partner_vectors_input_with_partners))]\n",
    ")\n",
    "\n",
    "train_amount, valid_amount, test_amount = dataset_split_ratio\n",
    "\n",
    "print('Loading dataset into train, valid, test')\n",
    "dataset_loading_start = time.time()\n",
    "\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test, class_weights = load_shuffled_train_valid_test(\n",
    "    filepaths=input_batch_filepaths, \n",
    "    ratios=dataset_split_ratio,\n",
    "    required_sample_amount=required_sample_amount,\n",
    "    flag_oversample=flag_oversample,\n",
    "    flag_undersample=flag_undersample,\n",
    "    sampling_target_amount=sampling_target_amount\n",
    ")\n",
    "\n",
    "dataset_loading_end = time.time()\n",
    "print('Loading finished in {}\\n'.format(dataset_loading_end-dataset_loading_start))\n",
    "\n",
    "print('Shape X_test: {}'.format(x_test.shape))\n",
    "print('Shape Y_test: {}\\n'.format(y_test.shape))\n",
    "\n",
    "print('Shape X_valid: {}'.format(x_valid.shape))\n",
    "print('Shape Y_valid: {}'.format(y_valid.shape))\n",
    "\n",
    "n_input, n_steps, n_classes = get_input_target_lengths(check_print=False)\n",
    "\n",
    "print(n_steps)\n",
    "print(n_input)\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural network constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RNN_1(\n",
    "    x: tf.placeholder,\n",
    "    n_input: int,\n",
    "    n_steps: int, \n",
    "    n_classes: int, \n",
    "    n_hidden_layers: int, \n",
    "    n_hidden_cells: int, \n",
    "    dropout_chance: float, \n",
    "    model_type: str,\n",
    "    train: bool):\n",
    "    \n",
    "    def get_weights(layer_from_size: int, layer_to_size: int):\n",
    "        \n",
    "        return tf.Variable(tf.random_normal([n_hidden_cells, n_classes]))\n",
    "    \n",
    "    def get_biases(layer_to_size: int):\n",
    "        \n",
    "        return tf.Variable(tf.random_normal([layer_to_size]))\n",
    "    \n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "    \n",
    "    if model_type == 'rnn':\n",
    "        cell_constructor = rnn.BasicRNNCell\n",
    "    elif model_type == 'gru':\n",
    "        cell_constructor = rnn.GRUCell\n",
    "    elif model_type == 'lstm':\n",
    "        cell_constructor = rnn.BasicLSTMCell\n",
    "    elif model_type == 'nas':\n",
    "        cell_constructor = rnn.NASCell\n",
    "    else:\n",
    "        raise Exception(\"model type not supported: {}\".format(model_type))\n",
    "    \n",
    "    cell = cell_constructor(n_hidden_cells, activation=tf.nn.relu)\n",
    "    if train:\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=1-dropout_chance)\n",
    "    \n",
    "    outputs, states = rnn.static_rnn(cell, x, dtype=tf.float32)\n",
    "    \n",
    "    weights_out = get_weights(n_hidden_cells, n_classes)\n",
    "    biases_out = get_biases(n_classes) \n",
    "    \n",
    "    output_layer = tf.matmul(outputs[-1], weights_out) + biases_out\n",
    "    \n",
    "    return output_layer\n",
    "\n",
    "def RNN_2(\n",
    "    x: tf.placeholder,\n",
    "    n_input: int,\n",
    "    n_steps: int, \n",
    "    n_classes: int, \n",
    "    n_hidden_layers: int, \n",
    "    n_hidden_cells: int, \n",
    "    dropout_chance: float, \n",
    "    model_type: str,\n",
    "    train: bool):\n",
    "    \n",
    "    def get_weights(layer_from_size: int, layer_to_size: int):\n",
    "        \n",
    "        return tf.Variable(tf.random_normal([n_hidden_cells, n_classes]))\n",
    "    \n",
    "    def get_biases(layer_to_size: int):\n",
    "        \n",
    "        return tf.Variable(tf.random_normal([layer_to_size]))\n",
    "    \n",
    "    tf.nn.dropout(x, 1-dropout_chance)\n",
    "\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "    \n",
    "    if model_type == 'rnn':\n",
    "        cell_constructor = rnn.BasicRNNCell\n",
    "    elif model_type == 'gru':\n",
    "        cell_constructor = rnn.GRUCell\n",
    "    elif model_type == 'lstm':\n",
    "        cell_constructor = rnn.BasicLSTMCell\n",
    "    elif model_type == 'nas':\n",
    "        cell_constructor = rnn.NASCell\n",
    "    else:\n",
    "        raise Exception(\"model type not supported: {}\".format(model_type))\n",
    "    \n",
    "    cell = cell_constructor(n_hidden_cells, activation=tf.nn.relu)\n",
    "    if train:\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=1-dropout_chance)\n",
    "        \n",
    "    multilayer_network = rnn.MultiRNNCell([cell] * n_hidden_layers)\n",
    "\n",
    "    outputs, states = rnn.static_rnn(multilayer_network, x, dtype=tf.float32)\n",
    "    \n",
    "    weights_in = get_weights(n_input, n_hidden_cells)\n",
    "    biases_in = get_biases(n_hidden_cells)\n",
    "    \n",
    "    weights_out = get_weights(n_hidden_cells, n_classes)\n",
    "    biases_out = get_biases(n_classes) \n",
    "    \n",
    "    output_layer = tf.matmul(outputs[-1], weights_out) + biases_out\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(\n",
    "    *, \n",
    "    n_hidden_layers: int, \n",
    "    n_hidden_cells: int,\n",
    "    learning_rate: float,\n",
    "    alpha: float,\n",
    "    min_epoch_amount: int,\n",
    "    batch_size: int,\n",
    "    dropout_chance: float,\n",
    "    model_type: str,\n",
    "    optimizer_type: str,\n",
    "    class_weights: tf.Tensor,\n",
    "    use_class_weights: bool,\n",
    "    train: bool):\n",
    "\n",
    "    x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "    logits = RNN_1(\n",
    "        x=x,\n",
    "        n_input=n_input,\n",
    "        n_steps=n_steps, \n",
    "        n_classes=n_classes, \n",
    "        n_hidden_layers=n_hidden_layers, \n",
    "        n_hidden_cells=n_hidden_cells,\n",
    "        dropout_chance=dropout_chance,\n",
    "        model_type=model_type,\n",
    "        train=train\n",
    "    )\n",
    "        \n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    \n",
    "    class_weights = tf.constant(class_weights)\n",
    "    class_weights = tf.reshape(class_weights, [1, n_classes])\n",
    "    weighted_logits = tf.multiply(logits, class_weights)\n",
    "    \n",
    "    if use_class_weights:\n",
    "        loss_function = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=weighted_logits,\n",
    "                labels=y\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        loss_function = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits,\n",
    "                labels=y\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer_constructor = tf.train.AdamOptimizer\n",
    "    elif optimizer_type == 'sgd':\n",
    "        optimizer_constructor = tf.train.GradientDescentOptimizer\n",
    "    elif optimizer_type == 'rmsp':\n",
    "        optimizer_constructor = tf.train.RMSPropOptimizer\n",
    "    elif optimizer_type == 'adg':\n",
    "        optimizer_constructor = tf.train.AdagradOptimizer\n",
    "    else:\n",
    "        raise ValueError('Value {} is not a valid string for declaring optimizer type'.format(optimizer_type))\n",
    "    \n",
    "    optimizer = optimizer_constructor(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_function)\n",
    "    \n",
    "    correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "\n",
    "        session.run(init)\n",
    "        \n",
    "        epoch = 0\n",
    "        epoch_valid_accuracies = []\n",
    "        stop_training = False\n",
    "\n",
    "        while (not stop_training) or (epoch < min_epoch_amount):\n",
    "            \n",
    "            epoch += 1\n",
    "            \n",
    "            print('\\nEpoch number {}\\n'.format(epoch))\n",
    "            \n",
    "            batch_train_accuracies = []\n",
    "            \n",
    "            x_train_shuffled = []\n",
    "            y_train_shuffled = []\n",
    "                        \n",
    "            shuffled_tuples = list(zip(x_train, y_train))\n",
    "            shuffle(shuffled_tuples)\n",
    "            \n",
    "            [x_train_shuffled.append(shuffled_pair[0]) for shuffled_pair in shuffled_tuples]\n",
    "            [y_train_shuffled.append(shuffled_pair[1]) for shuffled_pair in shuffled_tuples]\n",
    "                        \n",
    "            for index in range(0, len(x_train_shuffled), batch_size):\n",
    "                \n",
    "                x_batch_train = x_train_shuffled[index:index+batch_size]\n",
    "                y_batch_train = y_train_shuffled[index:index+batch_size]\n",
    "                \n",
    "                session.run(train_op, feed_dict={x: x_batch_train, y: y_batch_train})\n",
    "                \n",
    "                loss, batch_accuracy = session.run(\n",
    "                    [loss_function, accuracy],\n",
    "                    feed_dict={\n",
    "                        x: x_batch_train,\n",
    "                        y: y_batch_train\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                batch_train_accuracies.append(batch_accuracy)                    \n",
    "\n",
    "            print('\\nValidating after epoch: ')\n",
    "            \n",
    "            batch_valid_accuracies = []\n",
    "             \n",
    "            for index in range(0, len(x_valid), batch_size):\n",
    "                \n",
    "                x_batch_valid = x_valid[index:index+batch_size]\n",
    "                y_batch_valid = y_valid[index:index+batch_size]\n",
    "                \n",
    "                batch_valid_accuracies.append(\n",
    "                    session.run(accuracy, feed_dict={x: x_batch_valid, y:y_batch_valid})\n",
    "                )\n",
    "                \n",
    "            epoch_train_accuracy = 100*np.mean(batch_train_accuracies)\n",
    "            epoch_valid_accuracy = 100*np.mean(batch_valid_accuracies)\n",
    "            \n",
    "            print('Loss after epoch:    {0:.4f}'.format(loss))\n",
    "            print('Average train batch accuracy: {0:.4f}%'.format(epoch_train_accuracy))\n",
    "            print(\"Validation accuracy: {0:.4f}%\\n\".format(epoch_valid_accuracy))\n",
    "            \n",
    "            if (np.mean(epoch_valid_accuracies[-10:]) - epoch_valid_accuracy > 0.2) and (epoch >= min_epoch_amount):\n",
    "                \n",
    "                print('Stopping training because:')\n",
    "                print('/tAverage valid accuracy(current epoch): {0:.4f}%'.format(epoch_valid_accuracy))\n",
    "                print('/tAverage valid accuracy(last N epochs): {0:.4f}%'.format(np.mean(epoch_valid_accuracies[-10:])))\n",
    "\n",
    "                stop_training = True\n",
    "            \n",
    "            epoch_valid_accuracies.append(epoch_valid_accuracy)\n",
    "        \n",
    "        print('\\nFinal testing after all epochs: ')\n",
    "        \n",
    "        batch_test_accuracies = []\n",
    "        for index in range(0, len(x_test), batch_size):\n",
    "\n",
    "            x_batch_test = x_test[index:index+batch_size]\n",
    "            y_batch_test = y_test[index:index+batch_size]\n",
    "            \n",
    "            batch_test_accuracies.append(\n",
    "                session.run(accuracy, feed_dict={x: x_batch_test, y: y_batch_test})\n",
    "            )\n",
    "            \n",
    "        print(\"Test accuracy: {0:.4f}%\".format(100*np.mean(batch_test_accuracies)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:   34500 vectors\nTesting:    9500 vectors\nValidation: 5500 vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_test: (2500, 25, 423)\nShape Y_test: (2500, 7172)\nShape X_valid: (2500, 25, 423)\nShape Y_valid: (2500, 7172)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n423\n7172\n[1024, 1024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch number 1\n\nBatch number 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTesting after epoch: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 : 2827\n7 : 2730\n6 : 2703\n8 : 2711\n9 : 2638\n5 : 2762\n3 : 2632\n11 : 2708\n9 : 2643\n13 : 2724\n7 : 2683\n17 : 2862\n8 : 2807\n8 : 2677\n5 : 2766\n12 : 2740\n10 : 2670\n14 : 2933\n7 : 2807\n17 : 2846\n11 : 2878\n8 : 2843\n6 : 2699\n5 : 2735\n10 : 2838\n6 : 2722\n6 : 2660\n14 : 2907\n17 : 2733\n6 : 2917\n13 : 2690\n8 : 2647\n9 : 2635\n9 : 2659\n6 : 2783\n11 : 2715\n3 : 2752\n9 : 2740\n9 : 2668\n5 : 2950\n10 : 2630\n9 : 2741\n5 : 2678\n8 : 2656\n5 : 2655\n13 : 2721\n9 : 2697\n6 : 2804\n11 : 2708\n10 : 2670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.0025177815859852523\n\nEpoch number 2\n\nBatch number 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTesting after epoch: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 : 1829\n7 : 1822\n6 : 1812\n8 : 1818\n9 : 1822\n5 : 1825\n3 : 1810\n11 : 1819\n9 : 1832\n13 : 1824\n7 : 1821\n17 : 1820\n8 : 1827\n8 : 1821\n5 : 1808\n12 : 1821\n10 : 1814\n14 : 1843\n7 : 1845\n17 : 1825\n11 : 1820\n8 : 1833\n6 : 1824\n5 : 1843\n10 : 1838\n6 : 1832\n6 : 1823\n14 : 1839\n17 : 1831\n6 : 1833\n13 : 1818\n8 : 1830\n9 : 1832\n9 : 1829\n6 : 1841\n11 : 1835\n3 : 1821\n9 : 1820\n9 : 1805\n5 : 1833\n10 : 1835\n9 : 1835\n5 : 1818\n8 : 1833\n5 : 1828\n13 : 1817\n9 : 1821\n6 : 1827\n11 : 1807\n10 : 1820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.002473880368958798\n\nEpoch number 3\n\nBatch number 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTesting after epoch: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 : 1256\n7 : 1260\n6 : 1257\n8 : 1262\n9 : 1283\n5 : 1252\n3 : 1288\n11 : 1253\n9 : 1275\n13 : 1258\n7 : 1263\n17 : 1252\n8 : 1253\n8 : 1252\n5 : 1258\n12 : 1254\n10 : 1259\n14 : 1257\n7 : 1253\n17 : 1257\n11 : 1249\n8 : 1255\n6 : 1255\n5 : 1255\n10 : 1250\n6 : 1264\n6 : 1260\n14 : 1248\n17 : 1260\n6 : 1258\n13 : 1260\n8 : 1273\n9 : 1272\n9 : 1272\n6 : 1245\n11 : 1260\n3 : 1257\n9 : 1259\n9 : 1254\n5 : 1258\n10 : 1268\n9 : 1256\n5 : 1258\n8 : 1264\n5 : 1272\n13 : 1259\n9 : 1263\n6 : 1254\n11 : 1253\n10 : 1263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.0025209816464232225\n\nEpoch number 4\n\nBatch number 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTesting after epoch: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 : 829\n7 : 831\n6 : 827\n8 : 832\n9 : 840\n5 : 833\n3 : 841\n11 : 827\n9 : 837\n13 : 829\n7 : 829\n17 : 832\n8 : 830\n8 : 828\n5 : 830\n12 : 830\n10 : 831\n14 : 827\n7 : 831\n17 : 829\n11 : 828\n8 : 831\n6 : 831\n5 : 829\n10 : 827\n6 : 829\n6 : 830\n14 : 830\n17 : 832\n6 : 832\n13 : 835\n8 : 837\n9 : 834\n9 : 832\n6 : 829\n11 : 829\n3 : 831\n9 : 830\n9 : 830\n5 : 830\n10 : 833\n9 : 828\n5 : 829\n8 : 829\n5 : 833\n13 : 829\n9 : 830\n6 : 830\n11 : 831\n10 : 830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.0018191731526087803\n\nEpoch number 5\n\nBatch number 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTesting after epoch: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 : 466\n7 : 469\n6 : 469\n8 : 471\n9 : 475\n5 : 469\n3 : 478\n11 : 468\n9 : 470\n13 : 467\n7 : 472\n17 : 470\n8 : 469\n8 : 467\n5 : 474\n12 : 467\n10 : 471\n14 : 467\n7 : 468\n17 : 469\n11 : 470\n8 : 465\n6 : 467\n5 : 467\n10 : 466\n6 : 467\n6 : 474\n14 : 466\n17 : 468\n6 : 469\n13 : 472\n8 : 472\n9 : 470\n9 : 473\n6 : 469\n11 : 469\n3 : 469\n9 : 470\n9 : 471\n5 : 469\n10 : 471\n9 : 466\n5 : 470\n8 : 470\n5 : 473\n13 : 467\n9 : 471\n6 : 467\n11 : 467\n10 : 467\nF1 Score:  0.0019118966331681986\n\nEpoch number 6\n\nBatch number 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTesting after epoch: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 : 249\n7 : 248\n6 : 248\n8 : 248\n9 : 248\n5 : 249\n3 : 250\n11 : 248\n9 : 247\n13 : 248\n7 : 247\n17 : 250\n8 : 250\n8 : 248\n5 : 247\n12 : 248\n10 : 248\n14 : 249\n7 : 248\n17 : 248\n11 : 249\n8 : 249\n6 : 248\n5 : 247\n10 : 249\n6 : 248\n6 : 248\n14 : 250\n17 : 249\n6 : 249\n13 : 247\n8 : 247\n9 : 247\n9 : 248\n6 : 248\n11 : 248\n3 : 249\n9 : 249\n9 : 248\n5 : 251\n10 : 246\n9 : 249\n5 : 249\n8 : 247\n5 : 248\n13 : 248\n9 : 249\n6 : 248\n11 : 249\n10 : 248\nF1 Score:  0.0018825581643111366\n\nEpoch number 7\n\nBatch number 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTesting after epoch: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 : 123\n7 : 123\n6 : 123\n8 : 123\n9 : 123\n5 : 123\n3 : 123\n11 : 123\n9 : 123\n13 : 123\n7 : 124\n17 : 123\n8 : 123\n8 : 123\n5 : 124\n12 : 122\n10 : 123\n14 : 123\n7 : 123\n17 : 122\n11 : 123\n8 : 123\n6 : 123\n5 : 123\n10 : 122\n6 : 123\n6 : 123\n14 : 122\n17 : 123\n6 : 123\n13 : 123\n8 : 123\n9 : 124\n9 : 123\n6 : 122\n11 : 123\n3 : 123\n9 : 123\n9 : 123\n5 : 123\n10 : 124\n9 : 122\n5 : 123\n8 : 124\n5 : 123\n13 : 123\n9 : 123\n6 : 122\n11 : 123\n10 : 123\nF1 Score:  0.0024879102600687602\n\nEpoch number 8\n\nBatch number 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTesting after epoch: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 : 46\n7 : 46\n6 : 46\n8 : 45\n9 : 44\n5 : 46\n3 : 44\n11 : 46\n9 : 45\n13 : 46\n7 : 45\n17 : 46\n8 : 46\n8 : 45\n5 : 45\n12 : 45\n10 : 45\n14 : 45\n7 : 45\n17 : 45\n11 : 46\n8 : 45\n6 : 46\n5 : 45\n10 : 45\n6 : 46\n6 : 45\n14 : 45\n17 : 46\n6 : 46\n13 : 45\n8 : 45\n9 : 44\n9 : 44\n6 : 45\n11 : 46\n3 : 46\n9 : 46\n9 : 45\n5 : 46\n10 : 44\n9 : 45\n5 : 46\n8 : 44\n5 : 45\n13 : 46\n9 : 45\n6 : 45\n11 : 46\n10 : 46\nF1 Score:  0.003782173531279831\n\nEpoch number 9\n\nBatch number 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTesting after epoch: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 : 16\n7 : 16\n6 : 16\n8 : 16\n9 : 17\n5 : 16\n3 : 16\n11 : 16\n9 : 17\n13 : 16\n7 : 16\n17 : 16\n8 : 16\n8 : 16\n5 : 17\n12 : 16\n10 : 16\n14 : 16\n7 : 16\n17 : 16\n11 : 16\n8 : 16\n6 : 16\n5 : 16\n10 : 16\n6 : 16\n6 : 16\n14 : 16\n17 : 16\n6 : 16\n13 : 17\n8 : 17\n9 : 16\n9 : 16\n6 : 16\n11 : 16\n3 : 16\n9 : 16\n9 : 16\n5 : 16\n10 : 16\n9 : 16\n5 : 16\n8 : 16\n5 : 16\n13 : 16\n9 : 16\n6 : 16\n11 : 16\n10 : 16\nF1 Score:  0.0027069872663040046\n\nEpoch number 10\n\nBatch number 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTesting after epoch: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 : 4\n7 : 4\n6 : 4\n8 : 4\n9 : 4\n5 : 4\n3 : 4\n11 : 4\n9 : 4\n13 : 4\n7 : 4\n17 : 4\n8 : 4\n8 : 4\n5 : 4\n12 : 4\n10 : 4\n14 : 4\n7 : 4\n17 : 4\n11 : 4\n8 : 4\n6 : 4\n5 : 4\n10 : 4\n6 : 4\n6 : 4\n14 : 4\n17 : 4\n6 : 4\n13 : 4\n8 : 4\n9 : 4\n9 : 4\n6 : 4\n11 : 4\n3 : 4\n9 : 4\n9 : 4\n5 : 4\n10 : 4\n9 : 4\n5 : 4\n8 : 4\n5 : 4\n13 : 4\n9 : 4\n6 : 4\n11 : 4\n10 : 4\nF1 Score:  0.0\n\nEpoch number 11\n\nBatch number 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTesting after epoch: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 : 2\n7 : 2\n6 : 2\n8 : 2\n9 : 2\n5 : 2\n3 : 2\n11 : 2\n9 : 2\n13 : 2\n7 : 2\n17 : 2\n8 : 2\n8 : 2\n5 : 2\n12 : 2\n10 : 2\n14 : 2\n7 : 2\n17 : 2\n11 : 2\n8 : 2\n6 : 2\n5 : 2\n10 : 2\n6 : 2\n6 : 2\n14 : 2\n17 : 2\n6 : 2\n13 : 2\n8 : 2\n9 : 2\n9 : 2\n6 : 2\n11 : 2\n3 : 2\n9 : 2\n9 : 2\n5 : 2\n10 : 2\n9 : 2\n5 : 2\n8 : 2\n5 : 2\n13 : 2\n9 : 2\n6 : 2\n11 : 2\n10 : 2\nF1 Score:  0.0\n\nEpoch number 12\n\nBatch number 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTesting after epoch: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 : 0\n7 : 0\n6 : 0\n8 : 0\n9 : 0\n5 : 0\n3 : 0\n11 : 0\n9 : 0\n13 : 0\n7 : 0\n17 : 0\n8 : 0\n8 : 0\n5 : 0\n12 : 0\n10 : 0\n14 : 0\n7 : 0\n17 : 0\n11 : 0\n8 : 0\n6 : 0\n5 : 0\n10 : 0\n6 : 0\n6 : 0\n14 : 0\n17 : 0\n6 : 0\n13 : 0\n8 : 0\n9 : 0\n9 : 0\n6 : 0\n11 : 0\n3 : 0\n9 : 0\n9 : 0\n5 : 0\n10 : 0\n9 : 0\n5 : 0\n8 : 0\n5 : 0\n13 : 0\n9 : 0\n6 : 0\n11 : 0\n10 : 0\nF1 Score:  0.0\n\nEpoch number 13\n\nBatch number 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTesting after epoch: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ddca829c5803>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_thesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/DiplomaThesis/lstm.py\u001b[0m in \u001b[0;36mrun_thesis\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_predictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                 rounded_predictions.append(\n\u001b[0;32m--> 236\u001b[0;31m                     \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m                 )\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/DiplomaThesis/lstm.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_predictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                 rounded_predictions.append(\n\u001b[0;32m--> 236\u001b[0;31m                     \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m                 )\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mround_\u001b[0;34m(a, decimals, out)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \"\"\"\n\u001b[0;32m-> 2851\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36maround\u001b[0;34m(a, decimals, out)\u001b[0m\n\u001b[1;32m   2835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2836\u001b[0m     \"\"\"\n\u001b[0;32m-> 2837\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'round'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "run_training(\n",
    "    n_hidden_layers=1, \n",
    "    n_hidden_cells=128, \n",
    "    learning_rate=0.005, \n",
    "    alpha=0.5, \n",
    "    min_epoch_amount=20,\n",
    "    batch_size=250,\n",
    "    dropout_chance=0.5,\n",
    "    model_type='lstm',\n",
    "    optimizer_type='adam',\n",
    "    class_weights=class_weights,\n",
    "    use_class_weights=False,\n",
    "    train=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
